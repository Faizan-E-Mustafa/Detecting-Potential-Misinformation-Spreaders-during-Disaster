{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"./data/raw/en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"read data from xml files\"\"\"\n",
    "    \n",
    "    tweets_paths = [i for i in p.glob(\"*.xml\")]\n",
    "    targets_path = [i for i in p.glob(\"*.txt\")][0]\n",
    "    tweets = []\n",
    "    user_ids = []\n",
    "    # read tweets from xml files\n",
    "    for file_path in tweets_paths:\n",
    "        file_name = file_path.stem\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        documents = root.findall(\"./documents/\")\n",
    "        user_tweets = [doc.text for doc in documents]\n",
    "        \n",
    "        tweets.append(user_tweets)\n",
    "        user_ids.append(file_name)\n",
    "        \n",
    "    # get target mapping\n",
    "    with open(targets_path) as f:\n",
    "        content = f.read()\n",
    "        content = content.split(\"\\n\")\n",
    "        target_map = {}\n",
    "        for i in content:\n",
    "            try:\n",
    "                user_id, target = i.split(\":::\")\n",
    "            except:\n",
    "                continue\n",
    "            target_map[user_id] = int(target)\n",
    "    # prepare dataframe\n",
    "    df = pd.DataFrame({\n",
    "        \"user_id\":user_ids,\n",
    "        \"tweets\": tweets\n",
    "    })\n",
    "    df[\"target\"]= df.user_id.map(target_map)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(path = p)\n",
    "# df.tweets = df.tweets.apply(lambda x: \" \".join(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"./data/preprocessed/disaster/nlp_disaster.csv\")\n",
    "# df = df.rename({\"text\":\"tweets\"}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size = 0.2,\n",
    "                                     stratify = df.target.values,\n",
    "                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet_Selection:\n",
    "    \"\"\"Filter most important tweets using chi-square\"\"\"\n",
    "    \n",
    "    def fit(self, df_train, nimportant_words=4000):\n",
    "        \"\"\"get list of important words (keep_words)\"\"\"\n",
    "        \n",
    "        self.keep_words = self.get_n_important_words(df_train, N=nimportant_words)\n",
    "    \n",
    "    def get_n_important_words(self, df_train, N):\n",
    "\n",
    "        vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "        X_train = vectorizer.fit_transform(\n",
    "            [\" \".join(user_tweets) for user_tweets in df_train.tweets]\n",
    "        )\n",
    "\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        ch2 = SelectKBest(chi2, k=N)\n",
    "        X_train = ch2.fit_transform(X_train, df_train.target)\n",
    "\n",
    "        keep_words = np.array(feature_names)[ch2.get_support(indices=True)]\n",
    "        return keep_words\n",
    "    \n",
    "    def transform(self, df, keepn_tweets=300):\n",
    "        \"\"\"keep top n tweets in which important words occurs most frequently\"\"\"\n",
    "\n",
    "        selected_tweets, scores = self.select_topn_tweets(\n",
    "            df.tweets, self.keep_words, keep_topn=keepn_tweets\n",
    "        )\n",
    "\n",
    "        df[\"TopN_Tweets\"] = selected_tweets\n",
    "        df[\"Tweet_Scores\"] = scores\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def select_topn_tweets(self, tweet_list, keep_words, keep_topn):\n",
    "        \"\"\"Select top N tweets using Chi-Square\"\"\"\n",
    "\n",
    "        temp_tweet_list = []\n",
    "        temp_scores_list = []\n",
    "\n",
    "        for user_tweets in tqdm(tweet_list, total=len(tweet_list)):\n",
    "            tweet_scores = {}\n",
    "            for tweet_idx, tweet in enumerate(user_tweets):\n",
    "                tweet_score = 0\n",
    "                for word in tweet.split():\n",
    "                    if word.lower() in keep_words:\n",
    "                        tweet_score += 1\n",
    "\n",
    "                tweet_scores[tweet_idx] = tweet_score\n",
    "\n",
    "            selected_tweet_idx = list(\n",
    "                {\n",
    "                    k: v\n",
    "                    for k, v in sorted(\n",
    "                        tweet_scores.items(), key=lambda item: item[1], reverse=True\n",
    "                    )\n",
    "                }.keys()\n",
    "            )[:keep_topn]\n",
    "            selected_tweet_scores = list(\n",
    "                {\n",
    "                    k: v\n",
    "                    for k, v in sorted(\n",
    "                        tweet_scores.items(), key=lambda item: item[1], reverse=True\n",
    "                    )\n",
    "                }.values()\n",
    "            )[:keep_topn]\n",
    "            temp_user_tweets = np.array(user_tweets)[selected_tweet_idx]\n",
    "            temp_tweet_list.append(temp_user_tweets)\n",
    "            temp_scores_list.append(selected_tweet_scores)\n",
    "\n",
    "        return temp_tweet_list, temp_scores_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = Tweet_Selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.fit(train_df) \n",
    "\n",
    "train_df = ts.transform(train_df, keepn_tweets=30)\n",
    "test_df = ts.transform(test_df, keepn_tweets=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df, test_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle(\"./data/preprocessed/train_df.pkl\")\n",
    "test_df.to_pickle(\"./data/preprocessed/test_df.pkl\")\n",
    "full_df.to_pickle(\"./data/preprocessed/full_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
